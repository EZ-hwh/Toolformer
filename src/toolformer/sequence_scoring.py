from typing import List

import torch
import torch.functional as F

def pos_weighted(pos: torch.Tensor):
    hat_w = torch.max(torch.tensor(0), 1 - 0.2 * pos)
    return hat_w / torch.sum(hat_w)

def get_scores_for_labels(input: str, labels: str, model, tokenizer, causal_model, args):
    """
    Calculates the conditional log-likelihood of the labels given the input, for an encoder-decoder model.
    :param input:
        The input text
    :param labels:
        The labels
    :param model:
        The model to use
    :param tokenizer:
        The tokenizer to use
    :return:
    """
    if causal_model:
        input_enc = tokenizer(input, labels, return_tensors='pt', add_special_tokens=True, padding=True)
        #print(input_enc)
        target_ids = tokenizer(labels, return_tensors='pt', add_special_tokens=True, padding=True).input_ids
        #target_mask = torch.zeros_like(input_enc['input_ids']) 
        #target_mask[:,-target_ids.shape[1]:] = target_ids != tokenizer.pad_token_id
        input_enc = {k: v.to(args.device) for k, v in input_enc.items()}
        #target_enc = input_enc['input_ids'].clone()
        #target_enc[target_mask == 0] = -100
        
        with torch.no_grad():
            model_output = model(**input_enc)
            output_p = torch.softmax(model_output.logits[...,:-1,:], dim=-1).squeeze().gather(-1, input_enc['input_ids'][...,1:].permute(1,0)).squeeze() # Get probability of each target token
            output_p = output_p[-target_ids.shape[1]:].log()
            pos = pos_weighted(torch.arange(0, output_p.shape[0], device=output_p.device))
            weighted_ppls = -torch.sum(output_p * pos)
        return weighted_ppls
    """  else:
        # Get encodings
        input_enc = tokenizer.batch_encode_plus(input, return_tensors='pt', add_special_tokens=True, padding=True)
        target_enc = tokenizer.batch_encode_plus(labels, return_tensors='pt', padding='longest').input_ids
        input_enc = {k: v.to(model.device) for k, v in input_enc.items()}
        target_enc = target_enc.to(model.device)
        # Get encoder's last hidden state
        encoder_hidden_states = model.encoder(**input_enc)[0]

        # Repeat the inputs `num_labels` times
        encoder_hidden_states = encoder_hidden_states.unsqueeze(dim=1).repeat(1, num_labels, 1, 1).flatten(0, 1)
        attention_mask = input_enc.attention_mask.unsqueeze(dim=1).repeat(1, num_labels, 1).flatten(0, 1)
        
        # Create the decoding mask (that is generated by the T5 model at predict time) -- make it more efficient
        decoder_input_ids = torch.cat([torch.zeros((num_labels * batch_size, 1), dtype=torch.int), target_enc[:, :-1].repeat(num_labels, 1)], dim=1)
        decoder_attention_mask = (decoder_input_ids == decoder_input_ids).float()
        lm_target = target_enc - 100 * (target_enc == tokenizer.pad_token_id).long()

        model_output = model(
            attention_mask = attention_mask,
            encoder_outputs = [encoder_hidden_states],
            decoder_input_ids = decoder_input_ids,
            decoder_attention_mask = decoder_attention_mask,
        )

        # Compute the log probabilities associated with each of the labels
        labels_log_probs = F.cross_entropy(
            model_output.logits.flatten(0, 1),
            lm_target.repeat(num_labels, 1).flatten(0, 1),
            reduction="none"
        )

        # Sum log probs for each of the (input, label) pair
        labels_scores = labels_log_probs.view(batch_size, num_labels, -1)
        labels_scores = labels_scores.sum(dim=-1)

        # Note: Label log probabilities are positive (due to the internals of pytorch's
        # cross entropy). To obtain the "logits", we need to multiply by -1.
        return labels_scores * -1 """